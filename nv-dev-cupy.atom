<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://forums.developer.nvidia.com</id>
  <title>NVIDIA Developer Forums (cupy)</title>
  <updated>2022-07-06T06:12:16.873000+00:00</updated>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>https://forums.developer.nvidia.com/t/173024/1</id>
    <title>Free TensorRT GPU memory using Python API [1]</title>
    <updated>2021-03-22T19:02:53.759000+00:00</updated>
    <content>: ...ltiple times, I see that the GPU utilization increases of a few Mb each time, so maybe there is some kind of memory leak. I am getting measures using cupy free_bytes, total_bytes = cp.cuda.Device(0).mem_info . Here’s how I allocate my model: import pycuda.driver as cuda cuda.init() import cupy as cp imp...</content>
    <link href="https://forums.developer.nvidia.com/t/173024/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/173207/1</id>
    <title>Error: Some events were lost. How do I fix this? [1]</title>
    <updated>2021-03-24T10:51:32.574000+00:00</updated>
    <content>Jonathan Boyle: I’m profiling Python code which uses CUDA (e.g. CuPy) and always get an error about events being lost e.g. Some events (328,938) were lost. Certain charts (including CPU utilization) on the timeline may...</content>
    <link href="https://forums.developer.nvidia.com/t/173207/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/173658/1</id>
    <title>Nvidia-smi command not found despite installing CUDA [1]</title>
    <updated>2021-03-29T21:08:58.797000+00:00</updated>
    <content>David H Pitt: I installed CUDA on my Win10 machine this morning to use CuPy. Running nvcc --version gives an output, and so does conda list cudatoolkit. However, running nvidia-smi in a conda environment gives the error comma...</content>
    <link href="https://forums.developer.nvidia.com/t/173658/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/174053/1</id>
    <title>How can I create my custom containers for Jetson Nano [1]</title>
    <updated>2021-04-02T11:23:55.754000+00:00</updated>
    <content>: ...? l4t-tensorflow:r32.5.0-tf2.3-py3 TensorFlow 2.3.1 l4t-ml:r32.5.0-py3 TensorFlow 1.15 PyTorch v1.7.0 torchvision v0.8.0 torchaudio v0.7.0 onnx 1.8.0 CuPy 8.0.0 numpy 1.19.4 numba 0.52.0 OpenCV 4.4.1 pandas 1.1.5 scipy 1.5.4 scikit-learn 0.23.2 JupyterLab 2.2.9 Cheers</content>
    <link href="https://forums.developer.nvidia.com/t/174053/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/173773/2</id>
    <title>N Ways to SAXPY: Demonstrating the Breadth of GPU Programming Options [2]</title>
    <updated>2021-04-06T21:41:48.167000+00:00</updated>
    <content>dsingalNV: ...mance, allow better use of Unified Memory, etc. Furthermore, there have been more open source projects that allow one to program for NVIDIA GPUs like cuPy, Numba, Tensorflow, Pytorch and more are always showing up thanks to the https://developer.nvidia.com/cuda-llvm-compiler NVIDIA Compiler SDK Thanks f...</content>
    <link href="https://forums.developer.nvidia.com/t/173773/2" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/174959/7</id>
    <title>Unifying the CUDA Python Ecosystem [7]</title>
    <updated>2021-04-28T19:58:52.237000+00:00</updated>
    <content>Matt Nicely: ...to use the corresponding Driver/Runtime API. As far as question 2, I’m not sure at the moment. I’m pretty sure that functionality currently exists in CuPy.</content>
    <link href="https://forums.developer.nvidia.com/t/174959/7" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/169103/15</id>
    <title>Cupy crashes on Jetson Nano [15]</title>
    <updated>2021-04-29T13:58:35.876000+00:00</updated>
    <content>Alain Paillou: Hello Dustin, could this be an illustration of what you are talking about, but this time using cupy : // Copyright 2008-2021 Andreas Kloeckner // Copyright 2021 NVIDIA Corporation import pycuda.autoinit # noqa from pycuda.compiler import SourceModul...</content>
    <link href="https://forums.developer.nvidia.com/t/169103/15" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/178315/3</id>
    <title>CUDA in Python C/C++ extensions [3]</title>
    <updated>2021-05-20T15:48:12.796000+00:00</updated>
    <content>Matt Nicely: To mix CUDA/C++/Python, check out how CuPy accomplished this with NVRTC https://github.com/cupy/cupy/pull/3319 github.com/cupy/cupy Pull Request https://github.com/cupy/cupy/pull/3319 Support...</content>
    <link href="https://forums.developer.nvidia.com/t/178315/3" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/178647/2</id>
    <title>Unwrap: shift phase angles kernel implementation [2]</title>
    <updated>2021-05-24T13:09:21.586000+00:00</updated>
    <content>Matt Nicely: Have you looked at the CuPy implementation? https://github.com/cupy/cupy/blob/v9.0.0/cupy/_math/trigonometric.py#L115</content>
    <link href="https://forums.developer.nvidia.com/t/178647/2" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/179311/4</id>
    <title>Thank You NVIDIA - Everything is working fine on wsl2 and windows 10 [4]</title>
    <updated>2021-05-31T08:09:29.149000+00:00</updated>
    <content>K Glimps: ...yper, murmurhash, blis, numpy, requests, pathy, jinja2, packaging, catalogue Required-by: spacy-transformers mabd@LAPTOP-T8DQ9UK0:~$ i had to install cupy-cuda112 and here cuda 11.2 was important mabd@LAPTOP-T8DQ9UK0:~$ pip show cupy-cuda112 Name: cupy-cuda112 Version: 8.6.0 Summary: CuPy: A NumPy-compa...</content>
    <link href="https://forums.developer.nvidia.com/t/179311/4" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/179453/5</id>
    <title>Passing scalar to functions (cupy &amp;amp; pycuda) - scalar multiplication of a vector [5]</title>
    <updated>2021-05-31T21:25:41.467000+00:00</updated>
    <content>Zsolt Majzik: Thank you very very much! This is exactly what I need to study!</content>
    <link href="https://forums.developer.nvidia.com/t/179453/5" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/180564/1</id>
    <title>Gauss Rank Transformation Is 100x Faster with RAPIDS and CuPy [1]</title>
    <updated>2021-06-11T15:00:19.782000+00:00</updated>
    <content>Jen Witsoe: Originally published at: https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/ Gauss Rank Transformation Is 100x Faster with RAPIDS and CuPy | NVIDIA Developer Blog As explained in the Batch Normalization paper, training neural...</content>
    <link href="https://forums.developer.nvidia.com/t/180564/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/183295/2</id>
    <title>Efficient math functions on Xavier CPU [2]</title>
    <updated>2021-07-11T17:43:49.622000+00:00</updated>
    <content>Dominik: ...chapter/libc_19.html glibc . When using Python look at https://numpy.org/doc/stable/reference/routines.math.html Numpy math functions or https://docs.cupy.dev/en/stable/reference/math.html Cupy which speeds up computation by using GPU.</content>
    <link href="https://forums.developer.nvidia.com/t/183295/2" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/186501/2</id>
    <title>Computer Vision Python packages with Cuda-GPU support for Jetson AGX Xavier? [2]</title>
    <updated>2021-08-13T14:54:50.670000+00:00</updated>
    <content>dusty_nv: Hi @Tamas23 , the other popular ones I am familiar with are cupy (numpy for CUDA), pyCUDA (run CUDA kernels from Python), numba (autovectorization for GPU). I would check out the latest https://ngc.nvidia.com/catal...</content>
    <link href="https://forums.developer.nvidia.com/t/186501/2" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/158008/8</id>
    <title>MPI Multi-GPU process list in nvidia-smi [8]</title>
    <updated>2021-09-10T15:15:32.637000+00:00</updated>
    <content>Richard Elkins: ...device. Thank you and I will inform people about this issue thread. Sample configuration: OS : Linux-4.15.0-72-generic-x86_64-with-debian-stretch-sid CuPy Version : 8.6.0 NumPy Version : 1.21.1 SciPy Version : 1.7.0 Cython Build Version : 0.29.22 CUDA Root : /opt/conda CUDA Build Version : 11000 CUDA Dr...</content>
    <link href="https://forums.developer.nvidia.com/t/158008/8" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/189099/3</id>
    <title>CuPy installation on the Nano [3]</title>
    <updated>2021-09-14T07:16:59.021000+00:00</updated>
    <content>Yuvalg1987: Hi, I tried to compile it on a fresh image of the Nano and works!</content>
    <link href="https://forums.developer.nvidia.com/t/189099/3" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/184412/9</id>
    <title>Combine pycuda and cupy [9]</title>
    <updated>2021-09-20T21:58:40.975000+00:00</updated>
    <content>system: This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.</content>
    <link href="https://forums.developer.nvidia.com/t/184412/9" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/190439/1</id>
    <title>Linux kernel 5.10+ CUDA_ERROR_MISALIGNED_ADDRESS [1]</title>
    <updated>2021-09-29T17:09:28.989000+00:00</updated>
    <content>Dyckoe: ...on the cpu. Eventually everything freezes anyway. This happens much more quickly than case 1. Case 3: Trying to diagnose what is going on I insalled cupy and wrote a little python script that crunches some numbers on the gpu. import numpy as np import cupy as cp import time print('Running Test...') s =...</content>
    <link href="https://forums.developer.nvidia.com/t/190439/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/191013/1</id>
    <title>CuCIM I/O error [1]</title>
    <updated>2021-10-01T08:51:15.070000+00:00</updated>
    <content>Bilalbkr92: ...ptly shuts down while reading any image. I would like to know what needs to be done. Currently, I use CPU libraries to read images then convert it to CuPy arrays.</content>
    <link href="https://forums.developer.nvidia.com/t/191013/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/191108/4</id>
    <title>Invalid_handel [4]</title>
    <updated>2021-10-05T08:50:41.087000+00:00</updated>
    <content>Yuvalg1987: Hi, is seems that this issue is caused by the need to run CuPy and ZED in two different threads as they have different contexts.</content>
    <link href="https://forums.developer.nvidia.com/t/191108/4" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/191442/1</id>
    <title>Ubuntu20.04 + nvidia-driver-470 - card drops out after minimal use /delay after first cuda command but works w/nvidia-driver-460 [1]</title>
    <updated>2021-10-07T00:18:23.708000+00:00</updated>
    <content>Nick Cammorato: ...ning processes found | +-----------------------------------------------------------------------------+ works fine… at first. Can even dot matrixes in cupy. However after a few minutes (or hits of the driver, I can’t tell), it breaks / drops out and becomes a ‘no devices found’ that doesn’t respond to re...</content>
    <link href="https://forums.developer.nvidia.com/t/191442/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/192517/1</id>
    <title>Access gstreamer output on GPU for fast inference [1]</title>
    <updated>2021-10-20T03:26:28.261000+00:00</updated>
    <content>: ...if there is any component that I can use to replace appsink that would allow me to access the frame directly on the GPU using any cuda library (e.g. cupy , pycuda ).</content>
    <link href="https://forums.developer.nvidia.com/t/192517/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/191833/7</id>
    <title>Problem to Install TensorFlow&amp;lt;2.0 on jetson xavier [7]</title>
    <updated>2021-10-26T07:16:51.305000+00:00</updated>
    <content>AastaLLL: ...do -H pip3 install scipy==1.5 # Numba sudo apt-get install -y llvm-8 llvm-8-dev sudo -H LLVM_CONFIG=/usr/bin/llvm-config-8 pip3 install numba==0.48 # CuPy Thanks.</content>
    <link href="https://forums.developer.nvidia.com/t/191833/7" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/193826/2</id>
    <title>Using CURAND inside NVRTC (JIT-compiled) kernels [2]</title>
    <updated>2021-11-02T17:46:00.198000+00:00</updated>
    <content>Robert_Crovella: ...sume you are referring to the device API. For that, if it were me, I would use https://github.com/NVIDIA/jitify/issues/43 jitify . For example, AFAIK cupy uses this method (jitify) to enable support for curand device API in cupy user-defined kernels.</content>
    <link href="https://forums.developer.nvidia.com/t/193826/2" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/193860/1</id>
    <title>How to access cuda array on gpu in Python [1]</title>
    <updated>2021-11-02T23:45:30.835000+00:00</updated>
    <content>: ...hon 3.6 and DeepStream 5.1. I would like to decode an Rtsp video stream and to access the decoded frames on the gpu using libraries such as pycuda or cupy (other libraries might work too). I don’t want to access numpy array because I don’t want to copy the data to the cpu. It is my understanding that nv...</content>
    <link href="https://forums.developer.nvidia.com/t/193860/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/193426/5</id>
    <title>Running ROS2 Galactic and Cupy on a Docker on the Jetson Xavier NX [5]</title>
    <updated>2021-11-17T06:38:46.590000+00:00</updated>
    <content>system: This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.</content>
    <link href="https://forums.developer.nvidia.com/t/193426/5" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/196073/1</id>
    <title>How could I accelerate FFT on Nano? [1]</title>
    <updated>2021-11-24T06:10:59.633000+00:00</updated>
    <content>: ...on Nano, and I currently use the scipy.fftpack.fft()。 But the speed is so slow and I want to utilize the GPU to accelerate this process. I have tried cupy, but it takes more time than before. Does there exist any other way to do FFT on GPU in Nano? I know that pycuda could, but implement a FFT in C seem...</content>
    <link href="https://forums.developer.nvidia.com/t/196073/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/196698/1</id>
    <title>Deserialize engine failed because file path: &amp;hellip;/resnet10.caffemodel_b1_gpu0_int8.engine open error [1]</title>
    <updated>2021-11-30T23:43:57.450000+00:00</updated>
    <content>: ...orama==0.3.7 command-not-found==0.3 configobj==5.0.6 constantly==15.1.0 contextlib2==0.6.0.post1 contextvars==2.4 cronex==0.1.3.1 cryptography==2.1.4 cupy-cuda113==9.6.0 cycler==0.11.0 Cython==0.29.24 dataclasses==0.8 decorator==5.1.0 dill==0.3.4 distro-info===0.18ubuntu0.18.04.1 docutils==0.14 ec2-hibi...</content>
    <link href="https://forums.developer.nvidia.com/t/196698/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/56757/15</id>
    <title>Cuda sha256 calculations improvements [15]</title>
    <updated>2021-12-10T09:53:44.384000+00:00</updated>
    <content>Viddi Mardiansyah: Thank you very much for your quick response. I will be trying hard to figure this out. Will try with pycuda or numba or cupy.</content>
    <link href="https://forums.developer.nvidia.com/t/56757/15" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/199347/1</id>
    <title>Nearly all cuQuantum-python tests fail (installed using conda on Fedora 35) [1]</title>
    <updated>2021-12-31T22:12:20.946000+00:00</updated>
    <content>Steve Jeffrey: ...onda-forge attrs 21.2.0 pyhd3eb1b0_0 ca-certificates 2021.10.26 h06a4308_2 certifi 2021.10.8 py37h06a4308_0 cudatoolkit 11.5.0 h36ae40a_9 conda-forge cupy 10.0.0 py37hd2d9f0c_0 conda-forge cuquantum 0.1.0.30 h5c60f85_1 conda-forge cuquantum-python 0.1.0.0 py37hac9ef86_2 conda-forge cutensor 1.4.0.6 h753...</content>
    <link href="https://forums.developer.nvidia.com/t/199347/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/199790/19</id>
    <title>Cupy and loops [19]</title>
    <updated>2022-01-06T23:08:10.100000+00:00</updated>
    <content>: ...: This info, already included in the error output, could be your starting point to get more information: Thanks, I saw that, and now after installing cupy also in in the root environment using sudo works.</content>
    <link href="https://forums.developer.nvidia.com/t/199790/19" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/161486/7</id>
    <title>I compiled opencv 4.5 but did not update Python-Opencv [7]</title>
    <updated>2022-01-15T03:07:01.962000+00:00</updated>
    <content>: ...sic platform you want. JetPack 4.6 (L4T R32.6.1) l4t-ml:r32.6.1-py3 TensorFlow 1.15.5 PyTorch v1.9.0 torchvision v0.10.0 torchaudio v0.9.0 onnx 1.8.0 CuPy 9.2.0 numpy 1.19.5 numba 0.53.1 OpenCV 4.5.0 (with CUDA) pandas 1.1.5 scipy 1.5.4 scikit-learn 0.23.2 JupyterLab 2.2.9 save your time on docker, firs...</content>
    <link href="https://forums.developer.nvidia.com/t/161486/7" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/203936/3</id>
    <title>Installing cupy on jetson nano [3]</title>
    <updated>2022-03-09T17:47:34.387000+00:00</updated>
    <content>system: This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.</content>
    <link href="https://forums.developer.nvidia.com/t/203936/3" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/204695/3</id>
    <title>CuPy or other parallel methods for GPU usage on TX2? [3]</title>
    <updated>2022-03-14T19:25:06.067000+00:00</updated>
    <content>system: This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.</content>
    <link href="https://forums.developer.nvidia.com/t/204695/3" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/203654/12</id>
    <title>Cupy or pycuda on Jetson Xavier NX [12]</title>
    <updated>2022-03-23T05:21:43.480000+00:00</updated>
    <content>system: This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.</content>
    <link href="https://forums.developer.nvidia.com/t/203654/12" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/204836/6</id>
    <title>Cupy and DNN_BACKENDCUDA -&amp;gt; All Fail on TX2 [6]</title>
    <updated>2022-03-23T06:32:36.460000+00:00</updated>
    <content>system: This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.</content>
    <link href="https://forums.developer.nvidia.com/t/204836/6" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/211069/1</id>
    <title>CUDA in l4t-base:r34.1 [1]</title>
    <updated>2022-04-13T09:48:06.225000+00:00</updated>
    <content>ambrose.maker: Hello, I would like to build docker image with l4t-base:r34.1. But seems my installation process (build CuPy) cannot find CUDA, with error like this: fatal error: cuda.h: No such file or directory And the nvcc command not found nvcc --version bash: nvcc: com...</content>
    <link href="https://forums.developer.nvidia.com/t/211069/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/211040/9</id>
    <title>PyTorch 1.11 for JetPack 5.0 DP? [9]</title>
    <updated>2022-04-25T19:05:17.965000+00:00</updated>
    <content>znmeb: ...ages on L4T-base and I want the latest stable binaries that are built by someone else whenever possible. My current Docker build only uses cusignal / cupy and torchaudio built from source, and that takes about 2.3 hours on a Xavier NX. The last time I built a PyTorch wheel I think it was 3.5 hours on th...</content>
    <link href="https://forums.developer.nvidia.com/t/211040/9" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/204142/4</id>
    <title>CUDA memory error calculating shap values although enough memory [4]</title>
    <updated>2022-04-25T23:20:29.127000+00:00</updated>
    <content>system: This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.</content>
    <link href="https://forums.developer.nvidia.com/t/204142/4" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/212739/2</id>
    <title>Nvc++ OpenACC runtime segfaults if Intel MKL (numpy) is already loaded [2]</title>
    <updated>2022-04-27T17:14:12.149000+00:00</updated>
    <content>Mat Colgrove: ...dding the flag “-nomp” when creating the shared object so NVOMP isn’t linked in. Alternately, if you can install numpy without MKL, or better yet use cupy so the Python code can be offloaded to the GPU as well. https://www.nvidia.com/en-us/glossary/data-science/numpy/ NVIDIA Data Science Glossary https:...</content>
    <link href="https://forums.developer.nvidia.com/t/212739/2" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/212436/21</id>
    <title>Nvidia Cuda Compiler not showing up in Linux 22.04 [21]</title>
    <updated>2022-05-03T10:25:07.377000+00:00</updated>
    <content>Johnnynuca14: ...nk the next update of cuda and drivers will be quite big. I am on kernel 5.17.5 and it is completely stable with these drivers using cudnn, tensorrt, cupy and cudf for deep learning.</content>
    <link href="https://forums.developer.nvidia.com/t/212436/21" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/210913/8</id>
    <title>Cupy Install for Jetson Xavier NX [8]</title>
    <updated>2022-05-11T03:46:52.479000+00:00</updated>
    <content>system: This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.</content>
    <link href="https://forums.developer.nvidia.com/t/210913/8" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/215440/1</id>
    <title>Low performance on Drive AGX Driver [1]</title>
    <updated>2022-05-24T14:24:41.578000+00:00</updated>
    <content>Muhammed Al Kadi: ...us DevKit (E3550) other SDK Manager Version 1.8.0.10363 other Host Machine Version native Ubuntu 18.04 other I managed recently to install our Python/CuPy based Radar signal processing framework on Drive AGX Xavier dev kit. After doing some comparisons to other GPUs, I’m surprised that Drive AGX is sign...</content>
    <link href="https://forums.developer.nvidia.com/t/215440/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/213644/2</id>
    <title>Processing camera samples from RTSP to Redis with ffmpeg [2]</title>
    <updated>2022-05-25T10:23:46.847000+00:00</updated>
    <content>dsingalNV: ...idth copying frames to the GPU, decoding, and then sending them out to be read via numpy. Numpy: Since numpy uses system memory, using something like cupy that can access the data on GPU memory itself would reduce the memory copy from GPU memory to system memory. For future reference, our technical blog...</content>
    <link href="https://forums.developer.nvidia.com/t/213644/2" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/215554/1</id>
    <title>Using NVSHMEM on a Python Library [1]</title>
    <updated>2022-05-25T10:37:29.415000+00:00</updated>
    <content>guilhermehartmannk8bwe: I currently provide a CUDA library for a client in a similar model as cupy. I was wondering if it is possible to launch this code over nvshmem and how would be the best method do it. Ideally I would like to wrap the nvshmem...</content>
    <link href="https://forums.developer.nvidia.com/t/215554/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/216751/2</id>
    <title>CUDA Python vs PyCUDA [2]</title>
    <updated>2022-06-07T15:40:39.532000+00:00</updated>
    <content>Robert_Crovella: .../unifying-the-cuda-python-ecosystem/ blog and the questions that follow it may be of interest. FWIW there are other python/CUDA methodologies. numba, cupy, CUDA python, and pycuda are some of the available approaches to tap into CUDA acceleration from Python. CUDA Python can interoperate with most or al...</content>
    <link href="https://forums.developer.nvidia.com/t/216751/2" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/216041/8</id>
    <title>[TensorRT] ERROR: 1: [resize.cu::performLinearKernelLaunch::457] Error Code 1: Cuda Runtime (invalid argument) [8]</title>
    <updated>2022-06-14T18:04:36.114000+00:00</updated>
    <content>system: This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.</content>
    <link href="https://forums.developer.nvidia.com/t/216041/8" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/219698/1</id>
    <title>Gpu-cpu memory copying is slower in WSL?! [1]</title>
    <updated>2022-07-04T14:13:59.257000+00:00</updated>
    <content>K Glimps: ...le bit slower roughly in the range some milliseconds slower. Also Python stuff caching and memory copying less than Windows. I tested Pycuda-PyopenCL-Cupy-Numba. for big arrays. these python stuff i don’t know they double cache or what. Now, I write my kernels and run them from Pycuda. Writing them in C...</content>
    <link href="https://forums.developer.nvidia.com/t/219698/1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/217721/4</id>
    <title>Windows 11 WSL2 CUDA (Windows 11 Home 22000.708, Nvidia Studio Driver 512.96) [4]</title>
    <updated>2022-07-05T08:48:31.840000+00:00</updated>
    <content>K Glimps: ...6_64/21.3/cuda/11.2/bin:$PATH For any CUDA stuff fist you have to define path to your CUDA installation That is normal. For, Numba, Pycuda, Pyopencl, cupy and I think the same for tensorflow</content>
    <link href="https://forums.developer.nvidia.com/t/217721/4" rel="alternate"/>
  </entry>
  <entry>
    <id>https://forums.developer.nvidia.com/t/219869/1</id>
    <title>How many kernel you can launch in wsl wiith respect to windows [1]</title>
    <updated>2022-07-06T06:12:16.873000+00:00</updated>
    <content>K Glimps: ...uch since then. BIG mistake of WRI. So, I played with python stuff in order “to couple” with sage and cython. Numba has a limited CUDA functionality. CUPY is more oriented. They are nice within their field of application = makes life easier. Pycuda and Pyopencl were different for me, having a Ryzen 7, I...</content>
    <link href="https://forums.developer.nvidia.com/t/219869/1" rel="alternate"/>
  </entry>
</feed>
